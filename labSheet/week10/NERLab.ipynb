{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from itertools import combinations\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check out the data\n",
    "data = pd.read_csv('./input/snopes.csv')\n",
    "\n",
    "# TODO: remove unnecessary columns\n",
    "\n",
    "# make sure the data are strings (Pandas converts into numeric sometimes)\n",
    "data['claim'] = data['claim'].astype(str)\n",
    "\n",
    "#Display the data\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: process the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claims = []\n",
    "# remove duplicate claims (Not really needed since dropped already)\n",
    "# TODO\n",
    "\n",
    "# Eventually add lower and whitespace strip to see the impact on the result\n",
    "# TODO\n",
    "\n",
    "# use spaCy to process the data (disable tagger and parser)\n",
    "# put the results in the 'corpus' variable\n",
    "# TODO\n",
    "corpus = ... TODO ...\n",
    "\n",
    "# the result should be in the 'corpus' variable\n",
    "print('Number of claims: ', len(corpus))\n",
    "\n",
    "assert len(corpus) == 3122, \"Wrong processing of the corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first few claims, along with the entities identified\n",
    "# TODO (use corpus variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of times each entity appears in the corpus\n",
    "# Format: dictionnary {word(str): count(int)}\n",
    "all_ents = defaultdict(int)\n",
    "\n",
    "for i, doc in enumerate(corpus):\n",
    "    # TODO\n",
    "\n",
    "print('Number of distinct entities: ', len(all_ents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update this list of sentences and see the results of the pipeline\n",
    "# TODO: Find variants of sentences that are well/wrongly tagged\n",
    "my_sents = [u\"This is a sentence without named Entities\", \n",
    "            u\"This is a sentence without named Entities... apparently\"]\n",
    "\n",
    "for doc in list(nlp.pipe(my_sents)):\n",
    "    print(doc)\n",
    "    print(doc.ents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: identify most popular entities (most popular = most frequent)\n",
    "# NOTE: sort entities by freq\n",
    "sorted_ents = ... TODO ...\n",
    "print(sorted_ents[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many ents appear per claim?\n",
    "# TODO: calculate the average number of entities per claim?\n",
    "# TODO: draw a bar chart\n",
    "# NOTE: Use plt.hist\n",
    "\n",
    "plt.title('Entities per claim')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the cooccurrences of identified entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function counts cooccurrences of entities\n",
    "# Inputs: list of list of entities\n",
    "# Outputs: a dict of dict\n",
    "def coocurrence(*inputs):\n",
    "    com = defaultdict(int)\n",
    "    for named_entities in inputs:\n",
    "        # Build co-occurrence matrix\n",
    "        # TODO: create the cooc matrix com having the cooc counts of entities in named_entities\n",
    "        \n",
    "    # create the cooc dict\n",
    "    result = defaultdict(dict)\n",
    "    for (w1, w2), count in com.items():\n",
    "        if w1 != w2:\n",
    "            result[w1][w2] = {'count': count}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to filter out entities with less than min_count\n",
    "# Input: entities (dict of dict), min_count (int)\n",
    "# Output: \n",
    "def filter_ents_by_min_count(entities, min_count):\n",
    "    cooc_entities_filtered = defaultdict()\n",
    "    for k1, e in entities.items():\n",
    "        ents_over_x_count = {k2: v for k2, v in e.items() if v['count'] > min_count}\n",
    "        if ents_over_x_count:  # ie. Not empty\n",
    "            cooc_entities_filtered[k1] = ents_over_x_count\n",
    "    return cooc_entities_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# 1. make list of of entities in claims\n",
    "# 2. calculate co-occurrences of entities\n",
    "# 3. filter out entities cooccurring less than min_count time, use filter_ents_by_min_count\n",
    "# 4. print most frequent co-occuring entities\n",
    "min_count = 2\n",
    "\n",
    "# 1. make list of entities in claims\n",
    "claim_ents = []\n",
    "for doc in corpus:\n",
    "    string_ents = list(map(str, doc.ents))\n",
    "    claim_ents.append(string_ents)\n",
    "    \n",
    "# Keeping only claims with multiple entities\n",
    "multi_ent_claims = [c for c in claim_ents if len(c)>1]\n",
    "\n",
    "# 2. Creating the coocurrence dict\n",
    "cooc_entities = coocurrence(*multi_ent_claims)\n",
    "\n",
    "# 3. filter out entities cooccurring less than min_count time, use filter_ents_by_min_count\n",
    "filtered_entities = filter_ents_by_min_count(cooc_entities, min_count)\n",
    "\n",
    "# 4. Print most frequent co-occuring entities\n",
    "cooc_sum = defaultdict(int)\n",
    "for k1, e in filtered_entities.items():\n",
    "    for k2, v in e.items():\n",
    "        cooc_sum[k1] += v['count']\n",
    "\n",
    "sorted_cooc = sorted(cooc_sum.items(), key=operator.itemgetter(1), reverse=True)\n",
    "print('Most frequent Cooccurring entities:')\n",
    "sorted_cooc[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphing the coocurrence relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the data - eg top 30, including only ents with min weight 2\n",
    "top_n = 30\n",
    "min_count = 2\n",
    "figsize = (20, 15)\n",
    "scale_nodes = lambda x: (x * 30) + 1\n",
    "scale_edges = lambda x: 15 * x\n",
    "\n",
    "filtered_entities = filter_ents_by_min_count(coocur_entities, min_count)\n",
    "\n",
    "top_cooccur = [x[0] for x in sorted_coocur[:top_n]]  \n",
    "graph_edges = {k:filtered_edges[k] for k in top_cooccur}\n",
    "\n",
    "# Attempting to graph these top coocurrences\n",
    "graph = nx.from_dict_of_dicts(graph_edges)\n",
    "pos = nx.kamada_kawai_layout(graph)\n",
    "\n",
    "# Normalise, then scale the line weights\n",
    "weights = [graph[u][v]['count'] for u, v in graph.edges() if u != v]\n",
    "weights = list(map(lambda x: (x - min(weights)) / (max(weights) - min(weights)), weights))\n",
    "weights = list(map(scale_edges, weights))\n",
    "\n",
    "# Scale node weights \n",
    "sum_weights = [cooc_sum[n] if cooc_sum[n]>0 else 1 for n in graph.nodes]\n",
    "sum_weights = list(map(scale_nodes, sum_weights))\n",
    "# sum_weights = list(map(lambda x: 100*log(x), sum_weights))\n",
    "\n",
    "\n",
    "plt.figure(figsize=figsize)\n",
    "\n",
    "# nx.draw(G, pos)\n",
    "nx.draw_networkx_edges(graph, pos, alpha=0.2, width=weights)\n",
    "nx.draw_networkx_nodes(graph, pos, alpha=0.2, node_size=sum_weights)\n",
    "nx.draw_networkx_labels(graph, pos)\n",
    "\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "\n",
    "plt.title('Top coocurrences of named entities in Snopes claims')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra: create a wordcloud picture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud picture\n",
    "# use all the detected named entities\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "from PIL import Image\n",
    "\n",
    "# Join all the tweet entities into one list\n",
    "word_list = [j for i in claim_ents for j in i]\n",
    "\n",
    "# Count occurences of each entity \n",
    "word_count_dict=Counter(word_list)\n",
    "\n",
    "# Make the wordcloud\n",
    "mask = np.array(Image.open(\"images/twitter_mask.png\"))\n",
    "wordcloud = WordCloud(background_color=\"white\", max_words=100,\n",
    "                      width = 600, height = 300,\n",
    "                      mask=mask, contour_width=5, contour_color=\"skyblue\"\n",
    "                     )\n",
    "                      \n",
    "wordcloud.generate_from_frequencies(word_count_dict)\n",
    "\n",
    "# Show the wordcloud\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# Or save the file\n",
    "# plt.savefig('wordcloud.png', bbox_inches='tight')\n",
    "# plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
